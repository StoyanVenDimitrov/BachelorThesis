; This is an example configuration for training machine translation.  It is an
; INI file with few added syntactic restrictions.
;
; Names in square brackets refer to objects in the program. With the exception
; of the [main] block, all of them will be instantiated as objects.
;
; The field values can be of several types:
;
;   * None - interpreted as Python None
;   * True / False - interpreted as boolean values
;   * integers
;   * floating point numbers
;   * Python types (fully defined with module name)
;   * references to other objects in the configuration, closed in <>
;   * strings in quotes
;   * list of the previous, enclosed in square brackets, comma-separated
;


; The main block contains the mandatory fields for running an experiment.
; It is the only block that does not have the `class` parameter
[main]
name="translation"
output="test"
tf_manager=<tf_manager>

train_dataset=<train_data>
val_dataset=<val_data>

runners=[<runner>]
trainer=<trainer_mrt>
evaluation=[("target", evaluators.bleu.BLEU1), ("target", evaluators.bleu.BLEU4)]

batch_size=3
runners_batch_size=128
epochs=2

validation_period=100
logging_period=15
overwrite_output_dir=True
initial_variables=["/home/students/dimitrov/neuralmonkey/lt-exp-30k/lt-ep-bl/variables.data"]
start_temperature=0.00001 
lowering_temp_by=0.9

; The TF manager section configures TF flags and learning mode
; increasing number of session can be used for model ensembles
[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=16
num_sessions=1


; Below are definitions of the training and validation data objects.  Dataset
; is not a standard class, it treats the __init__ method's arguments as a
; dictionary, therefore the data series names can be any string, prefixed with
; "s_". To specify the output file for a series, use "s_" prefix and "_out"
; suffix, e.g.  "s_target_out" Series-level preprocessors can be specified by
; prefixing the resulting series name by `pre_`. Dataset-level preprocessors
; are entered separately as the `preprocessors` parameter with triples `raw`,
; `preprocessed`, `preprocessor`.
[train_data]
class=dataset.load_dataset_from_files
s_source="/home/students/dimitrov/neuralmonkey/data/ep-train/train.de"
s_target="/home/students/dimitrov/neuralmonkey/data/ep-train/train.en"
lazy=True
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]

[val_data]
class=dataset.load_dataset_from_files
s_source="/home/students/dimitrov/neuralmonkey/data/ep-dev/dev.de"
s_target="/home/students/dimitrov/neuralmonkey/data/ep-dev/dev.en"
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]


; Definition of byte-pair encoding preprocessor and postprocessor.
[bpe_preprocess]
class=processors.bpe.BPEPreprocessor
merge_file="/home/students/dimitrov/neuralmonkey/data/bpe_merges"

[bpe_postprocess]
class=processors.bpe.BPEPostprocessor


; Definition of the vocabulary. In this example, we use a shared vocabulary
; for both source and target language. As we are using BPE preprocessing,
; we can create the vocabulary from the BPE merge file
[shared_vocabulary]
class=vocabulary.from_bpe
path="/home/students/dimitrov/neuralmonkey/data/bpe_merges"


; This section defines the sentence encoder object.
[encoder]
class=encoders.recurrent.SentenceEncoder
name="encoder"
rnn_size=1024
max_input_len=60
embedding_size=500
dropout_keep_prob=0.8
data_id="source_bpe"
vocabulary=<shared_vocabulary>

[attention]
class=attention.Attention
name="attention_sentence_encoder"
encoder=<encoder>

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<encoder>]
rnn_size=1024
embedding_size=500
attentions=[<attention>]
dropout_keep_prob=0.8
data_id="target_bpe"
max_output_len=60
vocabulary=<shared_vocabulary>
conditional_gru=True

[trainer_mrt]
class=trainers.mrt_trainer.MinRiskTrainer
decoders=[<decoder>]
batch_size=3
num_of_samples=10
alpha=1.0
annealing=True
target_also_in_samples=False
postprocess=<bpe_postprocess>
clip_norm=1.0
optimizer=<optimizer>

[optimizer]
class=tf.train.AdamOptimizer
learning_rate=1.0e-5

; The definition of the greedy runner. It computes the decoding operations
; described in the graph.
[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=<bpe_postprocess>
