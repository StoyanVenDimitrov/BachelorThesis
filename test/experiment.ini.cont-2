[main]
name="translation"
output="test"
tf_manager=<tf_manager>
train_dataset=<train_data>
val_dataset=<val_data>
runners=[<runner>]
trainer=<trainer_mrt>
evaluation=[("target", evaluators.bleu.BLEU1), ("target", evaluators.bleu.BLEU4)]
batch_size=3
runners_batch_size=128
epochs=2
validation_period=100
logging_period=15
overwrite_output_dir=True
initial_variables=["/home/students/dimitrov/neuralmonkey/lt-exp-30k/lt-ep-bl/variables.data"]
start_temperature=0.00001
lowering_temp_by=0.9

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=16
num_sessions=1

[train_data]
class=dataset.load_dataset_from_files
s_source="/home/students/dimitrov/neuralmonkey/data/ep-train/train.de"
s_target="/home/students/dimitrov/neuralmonkey/data/ep-train/train.en"
lazy=True
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]

[val_data]
class=dataset.load_dataset_from_files
s_source="/home/students/dimitrov/neuralmonkey/data/ep-dev/dev.de"
s_target="/home/students/dimitrov/neuralmonkey/data/ep-dev/dev.en"
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]

[bpe_preprocess]
class=processors.bpe.BPEPreprocessor
merge_file="/home/students/dimitrov/neuralmonkey/data/bpe_merges"

[bpe_postprocess]
class=processors.bpe.BPEPostprocessor

[shared_vocabulary]
class=vocabulary.from_bpe
path="/home/students/dimitrov/neuralmonkey/data/bpe_merges"

[encoder]
class=encoders.recurrent.SentenceEncoder
name="encoder"
rnn_size=1024
max_input_len=60
embedding_size=500
dropout_keep_prob=0.8
data_id="source_bpe"
vocabulary=<shared_vocabulary>

[attention]
class=attention.Attention
name="attention_sentence_encoder"
encoder=<encoder>

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<encoder>]
rnn_size=1024
embedding_size=500
attentions=[<attention>]
dropout_keep_prob=0.8
data_id="target_bpe"
max_output_len=60
vocabulary=<shared_vocabulary>
conditional_gru=True

[trainer_mrt]
class=trainers.mrt_trainer.MinRiskTrainer
decoders=[<decoder>]
batch_size=3
num_of_samples=10
alpha=1.0
annealing=True
target_also_in_samples=False
postprocess=<bpe_postprocess>
clip_norm=1.0
optimizer=<optimizer>

[optimizer]
class=tf.train.AdamOptimizer
learning_rate=1.0e-5

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=<bpe_postprocess>

